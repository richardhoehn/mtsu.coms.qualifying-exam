{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da959ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/08 17:06:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Spark V:  3.3.2\n"
     ]
    }
   ],
   "source": [
    "# Setup Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get Imports Needed\n",
    "from pyspark.sql.functions import col, udf, regexp_replace, lower\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Get Datatypes needed for DataFrame manipulation\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "\n",
    "# Other Imports\n",
    "import re  # Import the \"re\" module for regular expressions\n",
    "\n",
    "\n",
    "# Setup Spark Session\n",
    "sc = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Qualyfing-Exam\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print Spark Version being run\n",
    "print(\"Spark V: \", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea0a3475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Row Count: 1500\n",
      "German Row Count: 1500\n",
      "English Train Row Count: 1288\n",
      "German Train Row Count: 1281\n",
      "English Extended Train Row Count: 2779\n",
      "German Extended Train Row Count: 2781\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  8.0|   81|\n",
      "|  7.0|  186|\n",
      "|  1.0|  142|\n",
      "|  4.0|   42|\n",
      "| 11.0|   25|\n",
      "| 10.0|  288|\n",
      "|  6.0|    8|\n",
      "|  5.0|  310|\n",
      "|  9.0|  206|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  8.0|  291|\n",
      "|  7.0|  271|\n",
      "|  1.0|  334|\n",
      "|  4.0|   62|\n",
      "| 11.0|  494|\n",
      "| 10.0|  375|\n",
      "|  6.0|  140|\n",
      "|  5.0|  499|\n",
      "|  9.0|  313|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the Dataframes for Training & Testing\n",
    "# Read the English & German Datasets into Dataframes\n",
    "df_en = sc.read.csv(\"data/pd_en_translated.csv\", header=True, inferSchema=True)\n",
    "df_de = sc.read.csv(\"data/pd_de_translated.csv\", header=True, inferSchema=True)\n",
    "print(f\"English Row Count: {df_en.count()}\")\n",
    "print(f\"German Row Count: {df_de.count()}\")\n",
    "\n",
    "# Ad Numerical Label for Emotions\n",
    "emotion_key = {\n",
    "    \"boredom\": 0,\n",
    "    \"love\": 1,\n",
    "    \"relief\": 2,\n",
    "    \"fun\": 3,\n",
    "    \"hate\": 4,\n",
    "    \"neutral\": 5,\n",
    "    \"anger\": 6,\n",
    "    \"happiness\": 7,\n",
    "    \"surprise\": 8,\n",
    "    \"sadness\": 9,\n",
    "    \"worry\": 10,\n",
    "    \"enthusiasm\": 11,\n",
    "    \"empty\": 12,\n",
    "    \"---\": 13\n",
    "}\n",
    "\n",
    "# Create a mapping function to map emotion_en to label\n",
    "def map_emotion(label):\n",
    "    return emotion_key[label]\n",
    "\n",
    "# Add a new column \"label\" to the DataFrame with numerical emotion labels\n",
    "map_emotion_udf = F.udf(map_emotion, IntegerType())\n",
    "df_en = df_en.withColumn(\"label\", map_emotion_udf(\"emotion_en\"))\n",
    "df_de = df_de.withColumn(\"label\", map_emotion_udf(\"emotion_en\"))\n",
    "\n",
    "# Cast the Label to Double\n",
    "df_en = df_en.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "df_de = df_de.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "\n",
    "\n",
    "# Make Sure there are No Rows with NULLs in the sentence columns\n",
    "df_en = df_en.dropna(subset=[\"sentence_en\"])\n",
    "df_en = df_en.dropna(subset=[\"sentence_de\"])\n",
    "\n",
    "df_de = df_de.dropna(subset=[\"sentence_de\"])\n",
    "df_de = df_de.dropna(subset=[\"sentence_en\"])\n",
    "\n",
    "\n",
    "# Function to clean Sentence\n",
    "def clean_sentence(sentence):\n",
    "    sentence = re.sub(r'@\\w+', '', sentence) # Remove mentions (@user)\n",
    "    sentence = re.sub(r'#\\w+', '', sentence) # Remove hashtags (#weekend)\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+|bit\\.ly/\\S+', '', sentence) # Remove URLs\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence) # Remove special characters and symbols\n",
    "    sentence = sentence.lower() # Convert to lowercase\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip() # Remove multiple spaces and leading/trailing spaces\n",
    "    sentence = re.sub(r'\\t', ' ', sentence) # Remove tabs\n",
    "    return sentence\n",
    "\n",
    "# Create a User-Defined Function (UDF) to apply the sentence cleaning function to the DataFrame\n",
    "clean_sentence_udf = udf(clean_sentence, StringType())\n",
    "df_en = df_en.withColumn(\"sentence_en\", clean_sentence_udf(\"sentence_en\"))\n",
    "df_en = df_en.withColumn(\"sentence_de\", clean_sentence_udf(\"sentence_de\"))\n",
    "\n",
    "df_de = df_de.withColumn(\"sentence_en\", clean_sentence_udf(\"sentence_en\"))\n",
    "df_de = df_de.withColumn(\"sentence_de\", clean_sentence_udf(\"sentence_de\"))\n",
    "\n",
    "\n",
    "# Split the English and German Dataframes for Training and Testing\n",
    "# We are using a 20/80 Split\n",
    "df_en_train, df_en_test = df_en.randomSplit([0.85, 0.15], seed=2023)\n",
    "df_de_train, df_de_test = df_de.randomSplit([0.85, 0.15], seed=2023)\n",
    "print(f\"English Train Row Count: {df_en_train.count()}\")\n",
    "print(f\"German Train Row Count: {df_de_train.count()}\")\n",
    "\n",
    "\n",
    "# Create the Extended Dataframe wiht Translated Data\n",
    "df_en_train_extended = df_en_train.union(df_de.select(*df_en_train.columns))\n",
    "df_de_train_extended = df_de_train.union(df_en.select(*df_de_train.columns))\n",
    "print(f\"English Extended Train Row Count: {df_en_train_extended.count()}\")\n",
    "print(f\"German Extended Train Row Count: {df_de_train_extended.count()}\")\n",
    "\n",
    "\n",
    "df_en_train.groupBy(\"label\").count().show()\n",
    "df_en_train_extended.groupBy(\"label\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97373240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy (A) - Random Forest: 0.2877 with an F1: 0.2074\n",
      "Test Accuracy (B) - Naive Bayes: 0.0566 with an F1: 0.0692\n",
      "Test Accuracy (C) - GLM: 0.2594 with an F1: 0.2592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Pass Values\n",
    "df_train = df_en_train\n",
    "df_test = df_en_test\n",
    "col_sentence = \"sentence_en\"\n",
    "\n",
    "\n",
    "# Initialize the AutoTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# Define a UDF to tokenize the text column\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "tokenize_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# Tokenize the text column using the UDF\n",
    "df_train = df_train.withColumn(\"words\", tokenize_udf(col_sentence))\n",
    "df_test = df_test.withColumn(\"words\", tokenize_udf(col_sentence))\n",
    "\n",
    "\n",
    "# Prepare the feature column by applying various transformations\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", vocabSize=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "# Create a pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[stop_words_remover, cv, idf])\n",
    "\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline_model = pipeline.fit(df_train)\n",
    "\n",
    "\n",
    "# Transform the training and testing data\n",
    "df_train = pipeline_model.transform(df_train)\n",
    "df_test = pipeline_model.transform(df_test)\n",
    "\n",
    "\n",
    "# Select relevant columns for the model\n",
    "df_train = df_train.select(\"features\", \"label\")\n",
    "df_test = df_test.select(\"features\", \"label\")\n",
    "\n",
    "\n",
    "# Train the RandomForestClassifier model\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "model_a = rf.fit(df_train)\n",
    "\n",
    "# Train the Naive Bayes model\n",
    "nb_model = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n",
    "model_b = nb_model.fit(df_train)\n",
    "\n",
    "# Train the GLM model (General Linear Model)\n",
    "glm_model = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=500)\n",
    "model_c = glm_model.fit(df_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_a = model_a.transform(df_test)\n",
    "predictions_b = model_b.transform(df_test)\n",
    "predictions_c = model_c.transform(df_test)\n",
    "\n",
    "\n",
    "# Evaluate the model's performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "accuracy_a = evaluator.evaluate(predictions_a)\n",
    "accuracy_b = evaluator.evaluate(predictions_b)\n",
    "accuracy_c = evaluator.evaluate(predictions_c)\n",
    "\n",
    "f1_a = evaluator_f1.evaluate(predictions_a)\n",
    "f1_b = evaluator_f1.evaluate(predictions_b)\n",
    "f1_c = evaluator_f1.evaluate(predictions_c)\n",
    "\n",
    "print()\n",
    "print(f\"Test Accuracy (A) - Random Forest: {accuracy_a:.4f} with an F1: {f1_a:.4f}\")\n",
    "print(f\"Test Accuracy (B) - Naive Bayes: {accuracy_b:.4f} with an F1: {f1_b:.4f}\")\n",
    "print(f\"Test Accuracy (C) - GLM: {accuracy_c:.4f} with an F1: {f1_c:.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02aa842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, MultilabelMetrics\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "metrics_a = MulticlassMetrics(predictions_a.select(\"prediction\", \"label\").rdd)\n",
    "metrics_b = MulticlassMetrics(predictions_b.select(\"prediction\", \"label\").rdd)\n",
    "metrics_c = MulticlassMetrics(predictions_c.select(\"prediction\", \"label\").rdd)\n",
    "\n",
    "confusion_matrix_a = metrics_a.confusionMatrix()\n",
    "confusion_matrix_b = metrics_b.confusionMatrix()\n",
    "confusion_matrix_c = metrics_c.confusionMatrix()\n",
    "\n",
    "# Display the results\n",
    "print(\"Confusion Matrix (A):\")\n",
    "print(confusion_matrix_a)\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix (B):\")\n",
    "print(confusion_matrix_b)\n",
    "print()\n",
    "\n",
    "print(\"Confusion Matrix (C):\")\n",
    "print(confusion_matrix_c)\n",
    "print()\n",
    "\n",
    "\n",
    "# Get additional metrics for each label\n",
    "labels_a = predictions_a.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "labels_b = predictions_b.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "labels_c = predictions_c.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for label in sorted(labels_a):\n",
    "    precision = metrics_a.precision(label)\n",
    "    recall = metrics_a.recall(label)\n",
    "    f1_score = metrics_a.fMeasure(label)\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print()\n",
    "    \n",
    "for label in sorted(labels_b):\n",
    "    precision = metrics_b.precision(label)\n",
    "    recall = metrics_b.recall(label)\n",
    "    f1_score = metrics_b.fMeasure(label)\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print()\n",
    "    \n",
    "for label in sorted(labels_c):\n",
    "    precision = metrics_c.precision(label)\n",
    "    recall = metrics_c.recall(label)\n",
    "    f1_score = metrics_c.fMeasure(label)\n",
    "    print(f\"Label: {label}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1_score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7f54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

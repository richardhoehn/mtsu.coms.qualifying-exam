{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fbe04a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark V:  3.3.2\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Setup Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get Spark Functions Needed\n",
    "from pyspark.sql.functions import col, udf, split, explode, lit\n",
    "\n",
    "# Get Datatypes needed for DataFrame manipulation\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, StringType\n",
    "\n",
    "# Setup Spark Session\n",
    "sc = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"data_clean_up\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print Spark Version being run\n",
    "print(\"Spark V: \", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8312132d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count (raw): 40000\n",
      "English Source Columns: ['tweet_id', 'sentiment', 'content']\n",
      "\n",
      "Count (filtered): 35692\n",
      "\n",
      "Grouped & Count by \"emotion\"\n",
      "+----------+-----+\n",
      "|emotion_en|count|\n",
      "+----------+-----+\n",
      "|love      |3842 |\n",
      "|hate      |1323 |\n",
      "|neutral   |8638 |\n",
      "|anger     |110  |\n",
      "|happiness |5209 |\n",
      "|surprise  |2187 |\n",
      "|sadness   |5165 |\n",
      "|worry     |8459 |\n",
      "|enthusiasm|759  |\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/data_en.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *********************************\n",
    "# *** English Data Preparations ***\n",
    "# *********************************\n",
    "\n",
    "# Load English CSV File into a Dataframe\n",
    "df_english = sc.read.csv(\"data/english.csv\", header=True, inferSchema=True)\n",
    "print(f'Count (raw): {df_english.count()}')\n",
    "\n",
    "# Print Columns\n",
    "print(f'English Source Columns: {df_english.columns}\\n')\n",
    "\n",
    "# Filter only lavbels that we mathc with the German counter parts\n",
    "filter_values = [\"love\", \"hate\", \"neutral\", \"anger\", \"happiness\", \"surprise\", \"sadness\", \"worry\", \"enthusiasm\"]\n",
    "df_english_filtered = df_english.filter(col(\"sentiment\").isin(filter_values))\n",
    "\n",
    "# Remove unnecessary column\n",
    "df_english_filtered = df_english_filtered.drop(\"tweet_id\")\n",
    "\n",
    "# Rename Columns\n",
    "df_english_filtered = df_english_filtered.withColumnRenamed(\"sentiment\", \"emotion_en\")\n",
    "df_english_filtered = df_english_filtered.withColumnRenamed(\"content\", \"sentence_en\")\n",
    "\n",
    "# Make sure the column order to the same for both german and english csv files\n",
    "df_english_filtered = df_english_filtered.select('sentence_en', 'emotion_en')\n",
    "\n",
    "print(f'Count (filtered): {df_english_filtered.count()}')\n",
    "\n",
    "# Group By for Details & Count\n",
    "df_english_grouped = df_english_filtered.groupBy('emotion_en').count()\n",
    "\n",
    "# Show Groupings and Respetive Counts\n",
    "print('\\nGrouped & Count by \"emotion\"')\n",
    "df_english_grouped.show(truncate=0)\n",
    "\n",
    "\n",
    "# Save Dataframe to CSV\n",
    "directory_path = 'data/spark_data_parts'\n",
    "df_english_filtered.coalesce(1).write.csv(directory_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "file_pattern = 'part-00000*.csv'\n",
    "file_path = glob.glob(directory_path + '/' + file_pattern)[0]\n",
    "\n",
    "shutil.move(file_path, './data/data_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec2eaf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count (raw): 1970\n",
      "German Source Columns: ['article_emotion', 'article_id', 'article_stance', 'paragraphs', 'snippet', 'source', 'title']\n",
      "\n",
      "Count (filtered): 2568\n",
      "\n",
      "Grouped & Count by \"emotion\"\n",
      "+------------+-----+\n",
      "|emotion_de  |count|\n",
      "+------------+-----+\n",
      "|Vertrauen   |316  |\n",
      "|Freude      |140  |\n",
      "|Ärger       |226  |\n",
      "|Überraschung|369  |\n",
      "|Traurigkeit |184  |\n",
      "|Antizipation|774  |\n",
      "|Unklar      |314  |\n",
      "|Angst       |154  |\n",
      "|Ekel        |29   |\n",
      "|Keine       |62   |\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/data_de.csv'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ********************************\n",
    "# *** German Data Preparations ***\n",
    "# ********************************\n",
    "\n",
    "# Load German JSON File into a Dataframe\n",
    "df_german = sc.read.json(\"data/german.json\")\n",
    "print(f'Count (raw): {df_german.count()}')\n",
    "\n",
    "# Print Columns\n",
    "print(f'German Source Columns: {df_german.columns}\\n')\n",
    "\n",
    "\n",
    "# We Need to \"split\" the \"artice_emotion\" column since the ETH team listed\n",
    "# multiple emotions in one column\n",
    "# In order to \"explode\" the column to it's distinct rows\n",
    "df_german_exploded = df_german.select('*', explode('article_emotion').alias('emotion_de'))\n",
    "\n",
    "# Rename Column\n",
    "df_german_exploded = df_german_exploded.withColumnRenamed(\"title\", \"sentence_de\")\n",
    "\n",
    "# Remove unnecessary column\n",
    "df_german_exploded = df_german_exploded.drop(\"article_id\")\n",
    "df_german_exploded = df_german_exploded.drop(\"article_stance\")\n",
    "df_german_exploded = df_german_exploded.drop(\"paragraphs\")\n",
    "df_german_exploded = df_german_exploded.drop(\"source\")\n",
    "df_german_exploded = df_german_exploded.drop(\"article_emotion\")\n",
    "df_german_exploded = df_german_exploded.drop(\"snippet\")\n",
    "\n",
    "# Make sure the column order to the same for both german and english csv files\n",
    "df_german_exploded = df_german_exploded.select('sentence_de', 'emotion_de')\n",
    "\n",
    "print(f'Count (filtered): {df_german_exploded.count()}')\n",
    "\n",
    "# Group By for Details & Count\n",
    "df_german_grouped = df_german_exploded.groupBy('emotion_de').count()\n",
    "\n",
    "# Show Groupings and Respetive Counts\n",
    "print('\\nGrouped & Count by \"emotion\"')\n",
    "df_german_grouped.show(truncate=0)\n",
    "\n",
    "\n",
    "# Save Dataframe to CSV\n",
    "directory_path = 'data/spark_data_parts'\n",
    "df_german_exploded.coalesce(1).write.csv(directory_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "file_pattern = 'part-00000*.csv'\n",
    "file_path = glob.glob(directory_path + '/' + file_pattern)[0]\n",
    "\n",
    "shutil.move(file_path, './data/data_de.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95519523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|emotion_en|  emotion_de|\n",
      "+----------+------------+\n",
      "|   boredom|         ---|\n",
      "|      love|   Vertrauen|\n",
      "|    relief|         ---|\n",
      "|       fun|         ---|\n",
      "|      hate|        Ekel|\n",
      "|   neutral|      Unklar|\n",
      "|     anger|       Ärger|\n",
      "| happiness|      Freude|\n",
      "|  surprise|Überraschung|\n",
      "|   sadness| Traurigkeit|\n",
      "|     worry|       Angst|\n",
      "|enthusiasm|Antizipation|\n",
      "|    empty |         ---|\n",
      "|       ---|       Keine|\n",
      "+----------+------------+\n",
      "\n",
      "+------------+--------------------+----------+----------+-----------+----------+\n",
      "|  emotion_de|         sentence_de|emotion_en|emotion_en|sentence_en|emotion_en|\n",
      "+------------+--------------------+----------+----------+-----------+----------+\n",
      "|   Vertrauen|Adoptiert zu sein...|      love|      love|       null|      love|\n",
      "|      Freude|Österreichs Verfa...| happiness| happiness|       null| happiness|\n",
      "|      Freude|Visana gewinnt er...| happiness| happiness|       null| happiness|\n",
      "|       Ärger|Masern: Erste Kin...|     anger|     anger|       null|     anger|\n",
      "|       Ärger|Der Staat ist sch...|     anger|     anger|       null|     anger|\n",
      "|       Ärger|Auch Islam-Kritik...|     anger|     anger|       null|     anger|\n",
      "|       Ärger|Im Online-Handel ...|     anger|     anger|       null|     anger|\n",
      "|       Ärger|Diese Sehnsucht n...|     anger|     anger|       null|     anger|\n",
      "|       Ärger|Frankreich rettet...|     anger|     anger|       null|     anger|\n",
      "|Überraschung|Noch nie arbeitet...|  surprise|  surprise|       null|  surprise|\n",
      "|Überraschung|Kuriose Preispoli...|  surprise|  surprise|       null|  surprise|\n",
      "| Traurigkeit|Von Viagra bis An...|   sadness|   sadness|       null|   sadness|\n",
      "|Antizipation|Die erneuerbaren ...|enthusiasm|enthusiasm|       null|enthusiasm|\n",
      "|      Unklar|Die Kinder von 50...|   neutral|   neutral|       null|   neutral|\n",
      "|      Unklar|«Es war nicht mei...|   neutral|   neutral|       null|   neutral|\n",
      "|      Unklar|Umweltrisiken am ...|   neutral|   neutral|       null|   neutral|\n",
      "|      Unklar|Veganes Essen per...|   neutral|   neutral|       null|   neutral|\n",
      "|      Unklar|Was ist an einem ...|   neutral|   neutral|       null|   neutral|\n",
      "|      Unklar|Die Kantonspolize...|   neutral|   neutral|       null|   neutral|\n",
      "|      Unklar|Dynamische Preise...|   neutral|   neutral|       null|   neutral|\n",
      "+------------+--------------------+----------+----------+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------+--------------------+------------+------------+-----------+------------+\n",
      "|emotion_en|         sentence_en|  emotion_de|  emotion_de|sentence_de|  emotion_de|\n",
      "+----------+--------------------+------------+------------+-----------+------------+\n",
      "|      love|@annarosekerr agreed|   Vertrauen|   Vertrauen|       null|   Vertrauen|\n",
      "|   neutral|@dannycastillo We...|      Unklar|      Unklar|       null|      Unklar|\n",
      "|   neutral|    cant fall asleep|      Unklar|      Unklar|       null|      Unklar|\n",
      "|   neutral|No Topic Maps tal...|      Unklar|      Unklar|       null|      Unklar|\n",
      "|  surprise|        Got the news|Überraschung|Überraschung|       null|Überraschung|\n",
      "|   sadness|Layin n bed with ...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|Funeral ceremony....| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|I should be sleep...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|@charviray Charle...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|@kelcouch I'm sor...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|Ugh! I have to be...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|@BrodyJenner if u...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|The storm is here...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|So sleepy again a...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|   sadness|How are YOU convi...| Traurigkeit| Traurigkeit|       null| Traurigkeit|\n",
      "|     worry|Re-pinging @ghost...|       Angst|       Angst|       null|       Angst|\n",
      "|     worry|Hmmm. http://www....|       Angst|       Angst|       null|       Angst|\n",
      "|     worry|Choked on her ret...|       Angst|       Angst|       null|       Angst|\n",
      "|     worry|@PerezHilton lady...|       Angst|       Angst|       null|       Angst|\n",
      "|     worry|@raaaaaaek oh too...|       Angst|       Angst|       null|       Angst|\n",
      "+----------+--------------------+------------+------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "23/07/19 10:32:24 ERROR Executor: Exception in task 0.0 in stage 260.0 (TID 659)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 1160, in processPartition\n",
      "    f(x)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_11323/2316904776.py\", line 64, in translate_de_to_en\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/google.py\", line 57, in translate\n",
      "    if is_input_valid(text, max_chars=5000):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/validate.py\", line 39, in is_input_valid\n",
      "    raise NotValidPayload(text)\n",
      "deep_translator.exceptions.NotValidPayload: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "23/07/19 10:32:24 WARN TaskSetManager: Lost task 0.0 in stage 260.0 (TID 659) (richards-mbp.localdomain executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 1160, in processPartition\n",
      "    f(x)\n",
      "  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_11323/2316904776.py\", line 64, in translate_de_to_en\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/google.py\", line 57, in translate\n",
      "    if is_input_valid(text, max_chars=5000):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/validate.py\", line 39, in is_input_valid\n",
      "    raise NotValidPayload(text)\n",
      "deep_translator.exceptions.NotValidPayload: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "23/07/19 10:32:24 ERROR TaskSetManager: Task 0 in stage 260.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 260.0 failed 1 times, most recent failure: Lost task 0.0 in stage 260.0 (TID 659) (richards-mbp.localdomain executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 1160, in processPartition\n    f(x)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_11323/2316904776.py\", line 64, in translate_de_to_en\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/google.py\", line 57, in translate\n    if is_input_valid(text, max_chars=5000):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/validate.py\", line 39, in is_input_valid\n    raise NotValidPayload(text)\ndeep_translator.exceptions.NotValidPayload: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 1160, in processPartition\n    f(x)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_11323/2316904776.py\", line 64, in translate_de_to_en\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/google.py\", line 57, in translate\n    if is_input_valid(text, max_chars=5000):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/validate.py\", line 39, in is_input_valid\n    raise NotValidPayload(text)\ndeep_translator.exceptions.NotValidPayload: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 68\u001b[0m\n\u001b[1;32m     64\u001b[0m     translated \u001b[38;5;241m=\u001b[39m GoogleTranslator(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mde\u001b[39m\u001b[38;5;124m'\u001b[39m, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranslate(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_en\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     65\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_en\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m translated\n\u001b[0;32m---> 68\u001b[0m \u001b[43mdf_german_exploded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranslate_de_to_en\u001b[49m\u001b[43m)\u001b[49m  \n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/sql/dataframe.py:901\u001b[0m, in \u001b[0;36mDataFrame.foreach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforeach\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable[[Row], \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    889\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m    890\u001b[0m \n\u001b[1;32m    891\u001b[0m \u001b[38;5;124;03m    This is a shorthand for ``df.rdd.foreach()``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m    >>> df.foreach(f)\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py:1163\u001b[0m, in \u001b[0;36mRDD.foreach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         f(x)\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[0;32m-> 1163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessPartition\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 260.0 failed 1 times, most recent failure: Lost task 0.0 in stage 260.0 (TID 659) (richards-mbp.localdomain executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 1160, in processPartition\n    f(x)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_11323/2316904776.py\", line 64, in translate_de_to_en\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/google.py\", line 57, in translate\n    if is_input_valid(text, max_chars=5000):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/validate.py\", line 39, in is_input_valid\n    raise NotValidPayload(text)\ndeep_translator.exceptions.NotValidPayload: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/rdd.py\", line 1160, in processPartition\n    f(x)\n  File \"/opt/homebrew/Cellar/apache-spark/3.3.2/libexec/python/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/8c/5_2vy2pn72x609w4xvq0yjzc0000gn/T/ipykernel_11323/2316904776.py\", line 64, in translate_de_to_en\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/google.py\", line 57, in translate\n    if is_input_valid(text, max_chars=5000):\n  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/deep_translator/validate.py\", line 39, in is_input_valid\n    raise NotValidPayload(text)\ndeep_translator.exceptions.NotValidPayload: None --> text must be a valid text with maximum 5000 character,otherwise it cannot be translated\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:758)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:740)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1021)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2278)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# **************************************\n",
    "# *** English <-> German Emotion Key ***\n",
    "# **************************************\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col, udf, split, explode, lit\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, StringType\n",
    "\n",
    "# Emotion Dictionary English <-> German\n",
    "# This key/value setup was previsoulsy created for linking purposes\n",
    "emotion_key = {\n",
    "    \"boredom\"    : \"---\",\n",
    "    \"love\"       : \"Vertrauen\",\n",
    "    \"relief\"     : \"---\",\n",
    "    \"fun\"        : \"---\",\n",
    "    \"hate\"       : \"Ekel\",\n",
    "    \"neutral\"    : \"Unklar\",\n",
    "    \"anger\"      : \"Ärger\",\n",
    "    \"happiness\"  : \"Freude\",\n",
    "    \"surprise\"   : \"Überraschung\", \n",
    "    \"sadness\"    : \"Traurigkeit\", \n",
    "    \"worry\"      : \"Angst\",\n",
    "    \"enthusiasm\" : \"Antizipation\", \n",
    "    \"empty \"     : \"---\",\n",
    "    \"---\"        : \"Keine\",\n",
    "}\n",
    "\n",
    "# Create the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"emotion_en\", StringType(), nullable=False),\n",
    "    StructField(\"emotion_de\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Convert the dictionary to a list of tuples\n",
    "data = [(key, value) for key, value in emotion_key.items()]\n",
    "\n",
    "# Create the PySaprk DataFrame\n",
    "df_emotion_key = sc.createDataFrame(data, schema)\n",
    "\n",
    "df_emotion_key.show()\n",
    "\n",
    "# Extend the English and German onto the datasets\n",
    "df_german_exploded = df_german_exploded.join(df_emotion_key, on=\"emotion_de\", how=\"left\")\n",
    "df_english_filtered = df_english_filtered.join(df_emotion_key, on=\"emotion_en\", how=\"left\")\n",
    "\n",
    "# New sentence column\n",
    "df_german_exploded = df_german_exploded.withColumn(\"sentence_en\", lit(None))\n",
    "df_english_filtered = df_english_filtered.withColumn(\"sentence_de\", lit(None))\n",
    "\n",
    "df_german_exploded.show()\n",
    "df_english_filtered.show()\n",
    "\n",
    "\n",
    "# Using Deep Translator to proxy to Google Translate\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "# Define Tranlation Functions\n",
    "def translate_en_to_de(row):\n",
    "    translated = GoogleTranslator(source='en', target='de').translate(row[\"sentence_en\"])\n",
    "    row[\"sentence_de\"] = translated\n",
    "\n",
    "def translate_de_to_en(row):\n",
    "    translated = GoogleTranslator(source='de', target='en').translate(row[\"sentence_de\"])\n",
    "    row[\"sentence_en\"] = translated\n",
    "    \n",
    "\n",
    "df_german_exploded.foreach(translate_de_to_en)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90720f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

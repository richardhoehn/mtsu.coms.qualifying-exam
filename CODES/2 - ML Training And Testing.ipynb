{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da959ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get Imports Needed\n",
    "from pyspark.sql.functions import col, udf, regexp_replace, lower\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Get Datatypes needed for DataFrame manipulation\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType\n",
    "\n",
    "# Other Imports\n",
    "import re  # Import the \"re\" module for regular expressions\n",
    "\n",
    "\n",
    "# Setup Spark Session\n",
    "sc = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"Qualyfing-Exam\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Print Spark Version being run\n",
    "print(\"Spark V: \", sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the Dataframes for Training & Testing\n",
    "# Read the English & German Datasets into Dataframes\n",
    "df_en = sc.read.csv(\"data/pd_en_translated.csv\", header=True, inferSchema=True)\n",
    "df_de = sc.read.csv(\"data/pd_de_translated.csv\", header=True, inferSchema=True)\n",
    "print(f\"English Row Count: {df_en.count()}\")\n",
    "print(f\"German Row Count: {df_de.count()}\")\n",
    "\n",
    "# Add Numerical Label for Emotions\n",
    "emotion_key = {\n",
    "    \"boredom\": 0,\n",
    "    \"love\": 1,\n",
    "    \"relief\": 2,\n",
    "    \"fun\": 3,\n",
    "    \"hate\": 4,\n",
    "    \"neutral\": 5,\n",
    "    \"anger\": 6,\n",
    "    \"happiness\": 7,\n",
    "    \"surprise\": 8,\n",
    "    \"sadness\": 9,\n",
    "    \"worry\": 10,\n",
    "    \"enthusiasm\": 11,\n",
    "    \"empty\": 12,\n",
    "    \"---\": 13\n",
    "}\n",
    "\n",
    "# Create a mapping function to map emotion_en to label\n",
    "def map_emotion(label):\n",
    "    return emotion_key[label]\n",
    "\n",
    "# Add a new column \"label\" to the DataFrame with numerical emotion labels\n",
    "map_emotion_udf = F.udf(map_emotion, IntegerType())\n",
    "df_en = df_en.withColumn(\"label\", map_emotion_udf(\"emotion_en\"))\n",
    "df_de = df_de.withColumn(\"label\", map_emotion_udf(\"emotion_en\"))\n",
    "\n",
    "# Cast the Label to Double\n",
    "df_en = df_en.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "df_de = df_de.withColumn(\"label\", col(\"label\").cast(\"double\"))\n",
    "\n",
    "\n",
    "# Make Sure there are No Rows with NULLs in the sentence columns\n",
    "df_en = df_en.dropna(subset=[\"sentence_en\"])\n",
    "df_en = df_en.dropna(subset=[\"sentence_de\"])\n",
    "\n",
    "df_de = df_de.dropna(subset=[\"sentence_de\"])\n",
    "df_de = df_de.dropna(subset=[\"sentence_en\"])\n",
    "\n",
    "\n",
    "# Function to clean Sentence\n",
    "def clean_sentence(sentence):\n",
    "    sentence = re.sub(r'@\\w+', '', sentence) # Remove mentions (@user)\n",
    "    sentence = re.sub(r'#\\w+', '', sentence) # Remove hashtags (#weekend)\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+|bit\\.ly/\\S+', '', sentence) # Remove URLs\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence) # Remove special characters and symbols\n",
    "    sentence = sentence.lower() # Convert to lowercase\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip() # Remove multiple spaces and leading/trailing spaces\n",
    "    sentence = re.sub(r'\\t', ' ', sentence) # Remove tabs\n",
    "    return sentence\n",
    "\n",
    "# Create a User-Defined Function (UDF) to apply the sentence cleaning function to the DataFrame\n",
    "clean_sentence_udf = udf(clean_sentence, StringType())\n",
    "df_en = df_en.withColumn(\"sentence_en\", clean_sentence_udf(\"sentence_en\"))\n",
    "df_en = df_en.withColumn(\"sentence_de\", clean_sentence_udf(\"sentence_de\"))\n",
    "\n",
    "df_de = df_de.withColumn(\"sentence_en\", clean_sentence_udf(\"sentence_en\"))\n",
    "df_de = df_de.withColumn(\"sentence_de\", clean_sentence_udf(\"sentence_de\"))\n",
    "\n",
    "\n",
    "# Split the English and German Dataframes for Training and Testing\n",
    "# We are using a 20/80 Split\n",
    "df_en_train, df_en_test = df_en.randomSplit([0.85, 0.15], seed=2023)\n",
    "df_de_train, df_de_test = df_de.randomSplit([0.85, 0.15], seed=2023)\n",
    "print(f\"English Train Row Count: {df_en_train.count()}\")\n",
    "print(f\"German Train Row Count: {df_de_train.count()}\")\n",
    "\n",
    "\n",
    "# Create the Extended Dataframe wiht Translated Data\n",
    "df_en_train_extended = df_en_train.union(df_de.select(*df_en_train.columns))\n",
    "df_de_train_extended = df_de_train.union(df_en.select(*df_de_train.columns))\n",
    "print(f\"English Extended Train Row Count: {df_en_train_extended.count()}\")\n",
    "print(f\"German Extended Train Row Count: {df_de_train_extended.count()}\")\n",
    "\n",
    "\n",
    "df_en_train.groupBy(\"label\").count().show()\n",
    "df_en_train_extended.groupBy(\"label\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97373240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, Tokenizer, StopWordsRemover, CountVectorizer, IDF, Word2Vec\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Clean Dataframes (becasue of past iterations)\n",
    "df_en_train = df_en_train.select(\"label\", \"sentence_en\", \"sentence_de\")\n",
    "df_en_test = df_en_test.select(\"label\", \"sentence_en\", \"sentence_de\")\n",
    "df_de_train = df_de_train.select(\"label\", \"sentence_en\", \"sentence_de\")\n",
    "df_de_test = df_de_test.select(\"label\", \"sentence_en\", \"sentence_de\")\n",
    "\n",
    "\n",
    "# Initialize the AutoTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# Define a UDF to tokenize the text column\n",
    "def tokenize_text(text):\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "tokenize_udf = udf(tokenize_text, ArrayType(StringType()))\n",
    "\n",
    "\n",
    "# Tokenize the text column using the UDF\n",
    "df_en_train = df_en_train.withColumn(\"words\", tokenize_udf(\"sentence_en\"))\n",
    "df_en_test = df_en_test.withColumn(\"words\", tokenize_udf(\"sentence_en\"))\n",
    "\n",
    "df_de_train = df_de_train.withColumn(\"words\", tokenize_udf(\"sentence_de\"))\n",
    "df_de_test = df_de_test.withColumn(\"words\", tokenize_udf(\"sentence_de\"))\n",
    "\n",
    "\n",
    "# Prepare the feature column by applying various transformations\n",
    "stop_words_remover = StopWordsRemover(inputCol=\"words\", \n",
    "                                      outputCol=\"filtered_words\")\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"filtered_words\", \n",
    "                     outputCol=\"raw_features\", \n",
    "                     vocabSize=1000)\n",
    "\n",
    "idf = IDF(inputCol=\"raw_features\", \n",
    "          outputCol=\"features\")\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100,\n",
    "                    minCount=3,\n",
    "                    inputCol=\"filtered_words\",\n",
    "                    outputCol=\"features\")\n",
    "\n",
    "# Models\n",
    "rfc = RandomForestClassifier(labelCol=\"label\", \n",
    "                             featuresCol=\"features\", \n",
    "                             maxBins=32,\n",
    "                             numTrees=100, \n",
    "                             maxDepth=10)\n",
    "\n",
    "nb = NaiveBayes(labelCol=\"label\", \n",
    "                featuresCol=\"features\", \n",
    "                modelType=\"multinomial\",\n",
    "                smoothing=1.0)\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", \n",
    "                        featuresCol=\"features\", \n",
    "                        regParam=1.0,\n",
    "                        elasticNetParam=0.01,\n",
    "                        maxIter=100)\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline_rfc = Pipeline(stages=[stop_words_remover, word2Vec, rfc ])\n",
    "pipeline_nb  = Pipeline(stages=[stop_words_remover, cv,       idf, nb])\n",
    "pipeline_lr  = Pipeline(stages=[stop_words_remover, cv,       idf, lr])\n",
    "\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "# This is essentially creating the model\n",
    "model_en_rfc = pipeline_rfc.fit(df_en_train)\n",
    "model_en_nb  = pipeline_nb.fit(df_en_train)\n",
    "model_en_lr  = pipeline_lr.fit(df_en_train)\n",
    "model_de_rfc = pipeline_rfc.fit(df_de_train)\n",
    "model_de_nb  = pipeline_nb.fit(df_de_train)\n",
    "model_de_lr  = pipeline_lr.fit(df_de_train)\n",
    "\n",
    "# Save Trained Models to Disk\n",
    "# These will later on be used by the RESTful API\n",
    "\n",
    "# First Cleanup Old Models\n",
    "for root, dirs, files in os.walk(\"./models/\", topdown=False):\n",
    "    for dir_name in dirs:\n",
    "        folder_path = os.path.join(root, dir_name)\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Removed folder: {folder_path}\")\n",
    "\n",
    "model_en_rfc.save(\"./models/model_en_rfc.model\")\n",
    "model_en_nb.save(\"./models/model_en_nb.model\")\n",
    "model_en_lr.save(\"./models/model_en_lr.model\")\n",
    "model_de_rfc.save(\"./models/model_de_rfc.model\")\n",
    "model_de_nb.save(\"./models/model_de_nb.model\")\n",
    "model_de_lr.save(\"./models/model_de_lr.model\")\n",
    "\n",
    "\n",
    "# Make Predictions\n",
    "predictions_en_rfc = model_en_rfc.transform(df_en_test)\n",
    "predictions_en_nb = model_en_nb.transform(df_en_test)\n",
    "predictions_en_lr = model_en_lr.transform(df_en_test)\n",
    "predictions_de_rfc = model_de_rfc.transform(df_de_test)\n",
    "predictions_de_nb = model_de_nb.transform(df_de_test)\n",
    "predictions_de_lr = model_de_lr.transform(df_de_test)\n",
    "\n",
    "\n",
    "# Evaluate the model's performance\n",
    "eval_ac = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "eval_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "# Process English\n",
    "accuracy_en_rfc = eval_ac.evaluate(predictions_en_rfc)\n",
    "accuracy_en_nb = eval_ac.evaluate(predictions_en_nb)\n",
    "accuracy_en_lr = eval_ac.evaluate(predictions_en_lr)\n",
    "\n",
    "f1_en_rfc = eval_f1.evaluate(predictions_en_rfc)\n",
    "f1_en_nb = eval_f1.evaluate(predictions_en_nb)\n",
    "f1_en_lr = eval_f1.evaluate(predictions_en_lr)\n",
    "\n",
    "# Process German\n",
    "accuracy_de_rfc = eval_ac.evaluate(predictions_de_rfc)\n",
    "accuracy_de_nb = eval_ac.evaluate(predictions_de_nb)\n",
    "accuracy_de_lr = eval_ac.evaluate(predictions_de_lr)\n",
    "\n",
    "f1_de_rfc = eval_f1.evaluate(predictions_de_rfc)\n",
    "f1_de_nb = eval_f1.evaluate(predictions_de_nb)\n",
    "f1_de_lr = eval_f1.evaluate(predictions_de_lr)\n",
    "\n",
    "print()\n",
    "print(\"English Dataset\")\n",
    "print(f\"Test Accuracy (RFC): {accuracy_en_rfc:.4f} with an F1: {f1_en_rfc:.4f}\")\n",
    "print(f\"Test Accuracy (NB): {accuracy_en_nb:.4f} with an F1: {f1_en_nb:.4f}\")\n",
    "print(f\"Test Accuracy (LR): {accuracy_en_lr:.4f} with an F1: {f1_en_lr:.4f}\")\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"German Dataset\")\n",
    "print(f\"Test Accuracy (RFC): {accuracy_de_rfc:.4f} with an F1: {f1_de_rfc:.4f}\")\n",
    "print(f\"Test Accuracy (NB): {accuracy_de_nb:.4f} with an F1: {f1_de_nb:.4f}\")\n",
    "print(f\"Test Accuracy (LR): {accuracy_de_lr:.4f} with an F1: {f1_de_lr:.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DE - Labels\")\n",
    "predictions_de_rfc.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"DE - RFC\")\n",
    "predictions_de_rfc.groupBy(\"prediction\").count().show()\n",
    "\n",
    "print(\"DE - LR\")\n",
    "predictions_de_lr.groupBy(\"prediction\").count().show()\n",
    "\n",
    "print(\"DE - NB\")\n",
    "predictions_de_nb.groupBy(\"prediction\").count().show()\n",
    "\n",
    "print(\"EN - Labels\")\n",
    "predictions_en_rfc.groupBy(\"label\").count().show()\n",
    "\n",
    "print(\"EN - RFC\")\n",
    "predictions_en_rfc.groupBy(\"prediction\").count().show()\n",
    "\n",
    "print(\"EN - LR\")\n",
    "predictions_en_lr.groupBy(\"prediction\").count().show()\n",
    "\n",
    "print(\"EN - NB\")\n",
    "predictions_en_nb.groupBy(\"prediction\").count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
